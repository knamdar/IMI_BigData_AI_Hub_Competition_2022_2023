{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DNUoVA1bOqZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from scipy import ndimage\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbQLziLi5p61"
   },
   "outputs": [],
   "source": [
    "root_path = # Fill in with absolute path to working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MHMAjyMYchqH",
    "outputId": "1d59de55-7870-48cc-8c7e-c98ebe3b6021"
   },
   "outputs": [],
   "source": [
    "# Preprocessing data\n",
    "raw_data = pd.read_excel(os.path.join(root_path, 'data/risk_sheet.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTPRBKCqhCGm"
   },
   "outputs": [],
   "source": [
    "# Dropping likely irrelevant columns (additional reasoning: dates are annoying to parse into features, and GENDER had too many nulls that weren't easily resolvable)\n",
    "data = raw_data.drop(['GENDER', 'CUSTOMER_ID', 'NAME', 'CUST_ADD_DT', 'OCPTN_NM'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EnNI9eSHiBjp"
   },
   "outputs": [],
   "source": [
    "non_ordinal_data_columns = ['RES_CNTRY_CA', 'CNTRY_OF_INCOME_CA', 'PEP_FL'] # to one hot encode\n",
    "ordinal_data_columns = ['BIRTH_DT', 'OCPTN_RISK', 'COUNTRY_RISK_INCOME', 'COUNTRY_RISK_RESIDENCY'] # to label encode\n",
    "normalize_data_columns = ['QUERY_RISK', 'CASH_SUM_IN', 'CASH_CNT_IN', 'CASH_SUM_OUT', 'CASH_CNT_IN', 'CASH_CNT_OUT', 'WIRES_SUM_IN', 'WIRES_CNT_IN', 'WIRES_SUM_OUT', 'WIRES_CNT_OUT'] # to normalize (min-max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xWS30lA8rfN"
   },
   "outputs": [],
   "source": [
    "data['BIRTH_DT'] = data['BIRTH_DT'].dt.strftime('%Y').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVIm1p5ErjZ-"
   },
   "outputs": [],
   "source": [
    "# Min-max scaling certain features\n",
    "minmax_scaler = MinMaxScaler()\n",
    "data[normalize_data_columns] = minmax_scaler.fit_transform(data[normalize_data_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inKqslNTsnPA"
   },
   "outputs": [],
   "source": [
    "# 1 indexing the ordinal columns when label encoding so that null values can be set to 0 to indicate that it is \"irrelevant information\" rather than having to remove these data samples\n",
    "for col in ordinal_data_columns:\n",
    "  if col == 'BIRTH_DT':\n",
    "    continue\n",
    "  data[col] = data[col].str.lower()\n",
    "  data[col] = data[col].replace('low', 1)\n",
    "  data[col] = data[col].replace('moderate', 2)\n",
    "  data[col] = data[col].replace('high', 3)\n",
    "  data[col].fillna(0, inplace = True)\n",
    "\n",
    "# RISK is the target variable and not only has slightly different names (moderate -> medium), but it also doesn't have any nulls, so can 0 index without concern\n",
    "data['RISK'] = data['RISK'].str.lower()\n",
    "data['RISK'] = data['RISK'].replace('low', 0)\n",
    "data['RISK'] = data['RISK'].replace('medium', 1)\n",
    "data['RISK'] = data['RISK'].replace('high', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJQ3JY-1MZC6"
   },
   "outputs": [],
   "source": [
    "def balance_dset(df, target):\n",
    "    high_risk_data = df[df[target]==2].copy()\n",
    "    labels = df[target].copy()\n",
    "    all_mid_risk = labels[labels == 1]\n",
    "    all_low_risk = labels[labels == 0]\n",
    "    mid_risk_to_keep = np.random.choice(all_mid_risk.index, size=high_risk_data.shape[0], replace=False)\n",
    "    low_risk_to_keep = np.random.choice(all_low_risk.index, size=high_risk_data.shape[0], replace=False)\n",
    "    mid_risk_data = df.iloc[mid_risk_to_keep].copy()\n",
    "    low_risk_data = df.iloc[low_risk_to_keep].copy()\n",
    "    new_df = pd.concat([high_risk_data, mid_risk_data, low_risk_data], axis=0)\n",
    "    return new_df\n",
    "\n",
    "data = balance_dset(data, \"RISK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4j3e8KUy6Zd"
   },
   "outputs": [],
   "source": [
    "data_col_names = list(data.columns.values)\n",
    "data_col_names.remove('RISK')\n",
    "labels = torch.from_numpy(data['RISK'].to_numpy()) # labels first because data is overridden in the next line\n",
    "data = torch.from_numpy(data[data_col_names].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqdF9fnJxBna"
   },
   "outputs": [],
   "source": [
    "# Prepare idxs, save if new, else load for consistency\n",
    "\n",
    "if not (os.path.exists(os.path.join(root_path, 'data/train_idxs.npy')) or os.path.exists(os.path.join(root_path, 'data/val_idxs.npy')) or os.path.exists(os.path.join(root_path, 'data/test_idxs.npy'))):\n",
    "  train_idxs, val_test_idxs = train_test_split(list(range(data.shape[0])), test_size=0.2)\n",
    "  val_idxs, test_idxs = train_test_split(val_test_idxs, test_size=0.5)\n",
    "  train_idxs = np.array(train_idxs)\n",
    "  val_idxs = np.array(val_idxs)\n",
    "  test_idxs = np.array(test_idxs)\n",
    "\n",
    "if not (os.path.exists(os.path.join(root_path, 'data/train_idxs.npy')) or os.path.exists(os.path.join(root_path, 'data/val_idxs.npy')) or os.path.exists(os.path.join(root_path, 'data/test_idxs.npy'))):\n",
    "  np.save(os.path.join(root_path, 'data/train_idxs.npy'), train_idxs)\n",
    "  np.save(os.path.join(root_path, 'data/val_idxs.npy'), val_idxs)\n",
    "  np.save(os.path.join(root_path, 'data/test_idxs.npy'), test_idxs)\n",
    "else:\n",
    "  train_idxs = np.load(os.path.join(root_path, 'data/train_idxs.npy'))\n",
    "  val_idxs = np.load(os.path.join(root_path, 'data/val_idxs.npy'))\n",
    "  test_idxs = np.load(os.path.join(root_path, 'data/test_idxs.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xSaOjkR_lC9"
   },
   "outputs": [],
   "source": [
    "# Preparing data\n",
    "data = data.squeeze().unsqueeze(1).float()\n",
    "train_data = data[train_idxs]\n",
    "val_data = data[val_idxs]\n",
    "train_labels = labels[train_idxs]\n",
    "val_labels = labels[val_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "checkpoint_path = \"task2_checkpoint.pt\"\n",
    "\n",
    "initial_learning_rate = 0.00002\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "epochs_per_summary = 1\n",
    "\n",
    "num_input_features = data.shape[1]\n",
    "num_initial_features = 200\n",
    "\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1m9WiWbXYaF"
   },
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "\n",
    "class EncoderModel(nn.Module):\n",
    "    def __init__(self, num_input_features, num_features):\n",
    "        super(EncoderModel, self).__init__()\n",
    "        # Blocks\n",
    "        #####################\n",
    "        in_0 = num_input_features\n",
    "        out_0 = num_initial_features\n",
    "        self.block_0 = nn.Sequential(\n",
    "            nn.Conv1d(in_0, out_0, kernel_size=3, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(out_0),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        in_1 = out_0\n",
    "        out_1 = math.floor(in_1 * 1.25)\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv1d(in_1, out_1, kernel_size=3, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(out_1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        in_2 = out_1\n",
    "        out_2 = math.floor(in_2 * 1.25)\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv1d(in_2, out_2, kernel_size=3, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(out_2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        in_3 = out_2\n",
    "        out_3 = math.floor(in_3 * 1.25)\n",
    "        self.block_3 = nn.Sequential(\n",
    "            nn.Conv1d(in_3, out_3, kernel_size=3, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(out_3),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        self.out_channels = [out_3, out_2, out_1, out_0] # To be used for decoders\n",
    "        #####################\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_0 = x\n",
    "        out_0 = self.block_0(in_0)\n",
    "\n",
    "        in_1 = out_0\n",
    "        out_1 = self.block_1(in_1)\n",
    "\n",
    "        in_2 = out_1\n",
    "        out_2 = self.block_2(in_2)\n",
    "\n",
    "        in_3 = out_2\n",
    "        out_3 = self.block_3(in_3)\n",
    "        \n",
    "        return out_3, out_2, out_1, out_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIyoUN2SfgdO"
   },
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "\n",
    "class DecoderModel(nn.Module):\n",
    "    def __init__(self, encoder_out_channels):\n",
    "        super(DecoderModel, self).__init__()\n",
    "        # Blocks\n",
    "        #####################\n",
    "        num_features = encoder_out_channels[0]\n",
    "        encoder_out_channels = encoder_out_channels[1:]\n",
    "\n",
    "        in_0 = num_features\n",
    "        out_0 = encoder_out_channels[0]\n",
    "        self.block_0 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_0, out_0, kernel_size=3, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(out_0),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        in_1 = encoder_out_channels[0]\n",
    "        out_1 = encoder_out_channels[1]\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_1, out_1, kernel_size=3, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(out_1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        in_2 = encoder_out_channels[1]\n",
    "        out_2 = encoder_out_channels[2]\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_2, out_2, kernel_size=3, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(out_2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        in_3 = encoder_out_channels[2]\n",
    "        out_3 = int(in_3 // 1.25)\n",
    "        self.block_3 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_3, out_3, kernel_size=3, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(out_3),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        # Skip connections\n",
    "        #####################\n",
    "        self.preskip_1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=encoder_out_channels[0], out_channels=encoder_out_channels[0], kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.skip_1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_1 * 2, out_channels=in_1, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.preskip_2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=encoder_out_channels[1], out_channels=encoder_out_channels[1], kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.skip_2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_2 * 2, out_channels=in_2, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.preskip_3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=encoder_out_channels[2], out_channels=encoder_out_channels[2], kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.skip_3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_3 * 2, out_channels=in_3, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        #####################\n",
    "\n",
    "    def forward(self, encoder_out_3, encoder_out_2, encoder_out_1, encoder_out_0):\n",
    "        in_0 = encoder_out_3\n",
    "        out_0 = self.block_0(encoder_out_3)\n",
    "\n",
    "        in_1 = self.skip_1(torch.cat((out_0, self.preskip_1(encoder_out_2)), 1))\n",
    "        out_1 = self.block_1(in_1)\n",
    "\n",
    "        in_2 = self.skip_2(torch.cat((out_1, self.preskip_2(encoder_out_1)), 1))\n",
    "        out_2 = self.block_2(in_2)\n",
    "\n",
    "        in_3 = self.skip_3(torch.cat((out_2, self.preskip_3(encoder_out_0)), 1))\n",
    "        out_3 = self.block_3(in_3)\n",
    "        \n",
    "        return out_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfYHcikb7v3O"
   },
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, num_input_features, num_initial_features):\n",
    "        super(CombinedModel, self).__init__()\n",
    "\n",
    "        self.encoder = EncoderModel(num_input_features, num_initial_features)\n",
    "        self.decoder_risk = DecoderModel(self.encoder.out_channels)\n",
    "        self.decoder_badactor = DecoderModel(self.encoder.out_channels)\n",
    "\n",
    "        flattened_size = 2560 # Just checked manually\n",
    "\n",
    "        self.linear_risk = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_size,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 3),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.linear_badactor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_size,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 2),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        #####################\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_out = self.encoder(x)\n",
    "        decoder_risk_out = self.decoder_risk(*encoder_out)\n",
    "        #decoder_badactor_out = self.decoder_badactor(*encoder_out)\n",
    "        risk_scores = self.linear_risk(decoder_risk_out)\n",
    "        #badactor_scores = self.linear_badactor(decoder_badactor_out)\n",
    "        \n",
    "        return risk_scores#, badactor_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4J4NhDDG9Bmh"
   },
   "outputs": [],
   "source": [
    "class classification_dataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "    # def __init__(self, data, tumor_labels):\n",
    "        super().__init__()\n",
    "        # data is (N, 4, H, W)\n",
    "        # labels is (N, H, W)\n",
    "\n",
    "        self.num_data = len(data)\n",
    "        self.data = data.float()\n",
    "        self.labels = labels.long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx] , self.labels[idx]\n",
    "\n",
    "\n",
    "train_dataset = classification_dataset(train_data, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, pin_memory=False, num_workers=0, drop_last=False, shuffle=True)\n",
    "val_dataset = classification_dataset(val_data, val_labels)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, pin_memory=False, num_workers=0, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "model = CombinedModel(num_input_features, num_initial_features)\n",
    "\n",
    "optimizer = torch.optim.Adam(lr=initial_learning_rate, params=model.parameters(), weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=num_epochs // 4, gamma=0.5)\n",
    "\n",
    "if use_cuda:\n",
    "    model = nn.DataParallel(model)\n",
    "    model = model.cuda()\n",
    "\n",
    "initial_epoch = 1\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    initial_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "start_time = time.time()\n",
    "epoch_start_time = time.time()\n",
    "tic = time.time()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Beginning training\")\n",
    "for epoch in range(initial_epoch, num_epochs + 1):\n",
    "\n",
    "    total_train_loss = 0\n",
    "    total_train_batches = 0\n",
    "\n",
    "    for batch, (data, labels) in enumerate(train_dataloader):\n",
    "        if use_cuda:\n",
    "            data = data.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        outs = model(data)\n",
    "\n",
    "        loss = cross_entropy_loss(outs, labels)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += float(loss)\n",
    "        total_train_batches += 1\n",
    "\n",
    "        if time.time() - tic > 10 * 60: # 10 minutes\n",
    "            checkpoint_state = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }\n",
    "            torch.save(checkpoint_state, checkpoint_path)\n",
    "            print(\"Checkpoint saved\")\n",
    "\n",
    "            tic = time.time()\n",
    "\n",
    "    total_train_loss /= total_train_batches\n",
    "    train_losses.append(total_train_loss)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        total_val_loss = 0\n",
    "        total_val_batches = 0\n",
    "\n",
    "        for batch, (data, labels) in enumerate(val_dataloader):\n",
    "            if use_cuda:\n",
    "                data = data.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            outs = model(data)\n",
    "\n",
    "            loss = cross_entropy_loss(outs, labels)\n",
    "\n",
    "            total_val_loss += float(loss)\n",
    "            total_val_batches += 1\n",
    "\n",
    "        total_val_loss /= total_val_batches\n",
    "        val_losses.append(total_val_loss)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if epoch % epochs_per_summary == epochs_per_summary - 1:\n",
    "        epoch_time = (time.time() - epoch_start_time) / epochs_per_summary\n",
    "\n",
    "        progress_msg = \"Epoch: {epoch}, Train Total Loss {total_loss}\".format(epoch=epoch, total_loss=total_train_loss)\n",
    "        progress_msg += \", Val Total Loss {total_loss}, Time {time}\".format(total_loss=total_val_loss, time=epoch_time)\n",
    "\n",
    "        print(progress_msg)\n",
    "\n",
    "        checkpoint_state = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint_state, checkpoint_path)\n",
    "        print(\"Checkpoint saved\")\n",
    "\n",
    "        tic = time.time()\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Completed training.\")\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(root_path, \"task2_model.pt\"))\n",
    "\n",
    "np.save(os.path.join(root_path, 'task2_train_losses.npy'), np.array(train_losses))\n",
    "np.save(os.path.join(root_path, 'task2_val_losses.npy'), np.array(val_losses))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
